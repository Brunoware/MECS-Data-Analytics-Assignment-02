# -*- coding: utf-8 -*-
"""Copy of [Student View] Recursos_Humanos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mp8UCZ1QZn4HNmoGr7pmREbsIxBLCqBh

![picture](https://drive.google.com/uc?export=view&id=1KaUW3_JAieeY7WVNfoRiTSr2JsaeAcXi)

# **Reclutamiento de Empleados con Aprendizaje Automático | Colocación Laboral con Python | Sistema de Adquisición de Talento en RR.HH.**

En el mundo acelerado de hoy, las empresas buscan constantemente formas innovadoras para optimizar sus procesos, y la gestión de recursos humanos (RR.HH.) no es una excepción. Los métodos de reclutamiento tradicionales suelen ser consumidores de tiempo y carecen de la eficiencia necesaria para identificar a los mejores candidatos para el trabajo. Sin embargo, con los avances en aprendizaje automático y análisis de datos, los profesionales de RR.HH. ahora tienen herramientas poderosas a su disposición para revolucionar el proceso de reclutamiento.

## **Entendimiento del Proyecto:**
El proyecto está diseñado para aprovechar las técnicas de ML para predecir colocaciones laborales basadas en varios factores, como el rendimiento académico, la experiencia laboral, la especialización, y más.

Al analizar los datos históricos de empleados anteriores, el sistema busca identificar patrones y correlaciones que puedan predecir si un candidato es probable que sea colocado o no.
"""

import numpy as np
import pandas as pd
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline

df = pd.read_csv("https://raw.githubusercontent.com/germaingarcia/Files_Lectures/main/Placement_Data_Full_Class.csv")

df.head()

"""## **Diccionario**

1. **Sl_no**: Número de serie o identificador único para cada entrada o persona.
2. **gender**: Género del individuo.
3. **ssc_p**: Porcentaje obtenido en los exámenes de secundaria (SSC suele referirse a Secondary School Certificate).
4. **ssc_b**: Junta o comité organizador de los exámenes de secundaria.
5. **hsc_p**: Porcentaje obtenido en los exámenes de educación superior secundaria (HSC se refiere a Higher Secondary Certificate).
6. **hsc_b**: Junta o comité organizador de los exámenes de educación superior secundaria.
7. **hsc_s**: Especialización o área de estudio en la educación superior secundaria (ciencias, comercio, arte, etc.).
8. **degree_p**: Porcentaje obtenido en el grado universitario.
9. **degree_t**: Tipo de grado o campo de estudio en la educación universitaria (por ejemplo, tecnología, artes, comercio).
10. **workex**: Experiencia laboral (generalmente indicado como 'Yes' o 'No').
11. **etest_p**: Porcentaje obtenido en un examen de prueba, posiblemente un examen de empleabilidad o habilidades específicas.
12. **specialisation**: Especialización en estudios de posgrado, especialmente en programas de MBA.
13. **mba_p**: Porcentaje obtenido en el programa de MBA.
14. **status**: Estado del empleo o resultado del proceso de selección (como 'employed' o 'unemployed').
15. **salary**: Salario ofrecido o actual del individuo.


"""

#Analizamos que variables no ayudan y lo eliminamos: 'ssc_b','hsc_b','hsc_s','degree_t','salary'
drop_columns = ['ssc_b','hsc_b','hsc_s','degree_t','salary']

df = df.drop(columns = drop_columns)

df.info()

"""## Preprocessing

## Encoding

 Limpiar y preparar el conjunto de datos para el análisis. Esto incluye manejar valores faltantes, codificar variables categóricas y escalar características numéricas.
"""

# codigica genero, experiencia de trabajo, estatus y especialización
# Puedes usar esta celda como punto de partida
cat_features = df.select_dtypes(include = 'object')\
                 .drop(columns = 'status')

cat_features.describe()

"""# Balance Dataset

"""

df['status'].value_counts()

from sklearn.utils import resample

class_0 = df[df['status'] == 'Placed'].copy()
class_1 = df[df['status'] == 'Not Placed'].copy()

class_1_bigger = resample(class_1,
                          replace = True,
                          n_samples = len(class_0),
                          random_state = 42)

balanced_data = pd.concat([class_0, class_1_bigger], axis = 0, ignore_index = True)\
                  .assign(status = lambda df: np.where(df['status'] == 'Not Placed', 1, 0))

balanced_data['status'].value_counts()

"""# Train Test Split"""

# Separación de feature y target
X = balanced_data.copy().drop('status', axis = 1)
y = balanced_data['status'].copy()

#Train Test Split con test_size=0.3
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True, random_state = 123, stratify = y)
# Puedes usar esta celda como punto de partida

"""# Feature Scaling"""

# scalar las características de x_train y x_test
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
# Puedes usar esta celda como punto de partida
num_features = X_train.select_dtypes(include = np.number)
num_features.describe()

# construct preprocessing
encoder = OneHotEncoder(drop = 'if_binary',
                        handle_unknown = 'ignore',
                        sparse_output = False)

scaler = MinMaxScaler()

transformers = make_column_transformer((encoder, cat_features.columns.tolist()),
                                       (scaler, num_features.columns.tolist()),
                                       remainder = 'drop')

"""# 1 k-nearest neighbor"""

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier()
knn_pipe = make_pipeline(transformers, model)
knn_pipe.fit(X_train, y_train)

# Matriz de confusión heatmap KNN
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
y_test_pred = knn_pipe.predict(X_test)

sns.heatmap(confusion_matrix(y_test, y_test_pred),
            annot = True);

# Matriz de confusión heatmap KNN
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

"""# Decision Tree"""

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
dt_pipe = make_pipeline(transformers, model)
dt_pipe.fit(X_train, y_train)

y_test_pred = dt_pipe.predict(X_test)

sns.heatmap(confusion_matrix(y_test, y_test_pred),
            annot = True);

# creating confusion matrix heatmap (Decision Tree)

"""# SVM"""

# Support Vector Machine
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
model = SVC()
sv_pipe = make_pipeline(transformers, model)
sv_pipe.fit(X_train, y_train)

y_test_pred = sv_pipe.predict(X_test)

sns.heatmap(confusion_matrix(y_test, y_test_pred),
            annot = True);

# creating confusion matrix heatmap SVM

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
rf_pipe = make_pipeline(transformers, model)
rf_pipe.fit(X_train, y_train)

y_test_pred = rf_pipe.predict(X_test)

sns.heatmap(confusion_matrix(y_test, y_test_pred),
            annot = True);

# creating confusion matrix heatmap - Random Forest

"""# Gaussian Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
gn_pipe = make_pipeline(transformers, model)
gn_pipe.fit(X_train, y_train)

y_test_pred = gn_pipe.predict(X_test)

sns.heatmap(confusion_matrix(y_test, y_test_pred),
            annot = True);

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
lr_pipe = make_pipeline(transformers, model)
lr_pipe.fit(X_train, y_train)

y_test_pred = lr_pipe.predict(X_test)

sns.heatmap(confusion_matrix(y_test, y_test_pred),
            annot = True);

"""## Single Input Predictions

Test si está prediciendo adecuadamente
"""

# construct preprocessing
encoder = OneHotEncoder(drop = 'if_binary',
                        handle_unknown = 'ignore',
                        sparse_output = False)
scaler = MinMaxScaler()

transformers = make_column_transformer((encoder, cat_features.columns.tolist()),
                                       (scaler, num_features.columns.tolist()),
                                       remainder = 'drop')

model = RandomForestClassifier()
rf_pipe = make_pipeline(transformers, model)
rf_pipe.fit(X_train, y_train)

sample = X_test.iloc[[0]]
y_sample = y_test.iloc[0]

prediction = rf_pipe.predict(sample)[0]

print(f'Predicted Class: {prediction}')
print(f'Actual Class: {y_sample}')



"""# Sistema de Predicción"""

def prediction(sl_no, gender, ssc_p, hsc_p, degree_p, workex, etest_p, specialisation, mba_p):

    return 0

sl_no = 11
gender = "F"
ssc_p = 58.
hsc_p = 61.
degree_p = 60.
workex = "Yes"
etest_p = 62.
specialisation = "Mkt&Fin"
mba_p = 60.85

# Crear el DataFrame
sample = pd.DataFrame({'sl_no': [sl_no],
                       'gender': [gender],
                       'ssc_p': [ssc_p],
                       'hsc_p': [hsc_p],
                       'degree_p': [degree_p],
                       'workex': [workex],
                       'etest_p': [etest_p],
                       'specialisation': [specialisation],
                       'mba_p': [mba_p]})

result = rf_pipe.predict(sample)[0]

if result == 0:
    print('contratado')

else:
    print('No Contratado')

"""Se emplea un modelo de Clasificador Random Forest para predecir las colocaciones laborales basadas en características de entrada como los puntajes académicos, la experiencia laboral y otros factores relevantes. El modelo se entrena con datos históricos para aprender patrones y hacer predicciones precisas.

# Guardar Archivos
"""

import pickle

with open('encoder.pkl', 'wb') as file:
    pickle.dump(encoder, file)

with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

with open('transformers.pkl', 'wb') as file:
    pickle.dump(transformers, file)

with open('model_pipe.pkl', 'wb') as file:
    pickle.dump(rf_pipe, file)

del encoder, scaler, transformers, rf_pipe

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer

with open('encoder.pkl', 'rb') as f:
    encoder = pickle.load(f)

with open('scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

with open('transformers.pkl', 'rb') as f:
    transformers = pickle.load(f)

with open('model_pipe.pkl', 'rb') as f:
    model_pipe = pickle.load(f)

sl_no = 11
gender = "F"
ssc_p = 58.
hsc_p = 61.
degree_p = 60.
workex = "Yes"
etest_p = 62.
specialisation = "Mkt&Fin"
mba_p = 60.85

# Crear el DataFrame
sample = pd.DataFrame({'sl_no': [sl_no],
                       'gender': [gender],
                       'ssc_p': [ssc_p],
                       'hsc_p': [hsc_p],
                       'degree_p': [degree_p],
                       'workex': [workex],
                       'etest_p': [etest_p],
                       'specialisation': [specialisation],
                       'mba_p': [mba_p]})

result = model_pipe.predict(sample)[0]

if result == 0:
    print('contratado')

else:
    print('No Contratado')